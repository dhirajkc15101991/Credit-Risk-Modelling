{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency,f_oneway\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,precision_score,recall_score,f1_score,classification_report,r2_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import warnings\n",
    "import os\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import json\n",
    "from sklearn.preprocessing import OneHotEncoder,OrdinalEncoder,StandardScaler,LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('c:/Users/dhiraj_choudhary/Music/Credit-Risk-Modelling/notebooks')"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('c:/Users/dhiraj_choudhary/Music/Credit-Risk-Modelling')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Go to base folder of the project from the current file deitory- one lave up\n",
    "root_folder_path=Path.cwd().parent\n",
    "root_folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('c:/Users/dhiraj_choudhary/Music/Credit-Risk-Modelling/data/raw_data')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Go to Raw data folder\n",
    "raw_data_folder=root_folder_path/'data'/'raw_data'\n",
    "raw_data_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the internal data as well as credit bearue one\n",
    "internal_data_cust=pd.read_excel(raw_data_folder/'Internal_data_Bank.xlsx')\n",
    "cibil_data_cust=pd.read_excel(raw_data_folder/'Cibil_Data_External.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utills file\n",
    "#create the function to remove df_internal_data_cust\n",
    "def null_removal_df_internal_data_cust(internal_data_cust):\n",
    "    df_internal_data_cust_final=internal_data_cust.loc[internal_data_cust['Age_Oldest_TL'] != -99999]\n",
    "    return df_internal_data_cust_final\n",
    "\n",
    "#create function for null removal of df_cibil_data_cust\n",
    "\n",
    "def null_removal_df_cibil_data_cust(cibil_data_cust):\n",
    "    threshold = 20 \n",
    "    # Loop through all columns and drop those with >20% -99999 values\n",
    "    columns_to_drop = []\n",
    "    for col in cibil_data_cust.columns:\n",
    "        null_percentage = (cibil_data_cust[col] == -99999).sum() / len(cibil_data_cust) * 100\n",
    "        if null_percentage > threshold:\n",
    "            columns_to_drop.append(col)\n",
    "    # Drop the identified columns\n",
    "    cibil_data_cust.drop(columns=columns_to_drop, inplace=True)\n",
    "    # Step 2: Drop rows where any remaining column contains -99999\n",
    "    df_cibil_data_cust_final = cibil_data_cust[cibil_data_cust.ne(-99999).all(axis=1)]\n",
    "    return df_cibil_data_cust_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_internal_data_cust=null_removal_df_internal_data_cust(internal_data_cust)\n",
    "df_cibil_data_cust=null_removal_df_cibil_data_cust(cibil_data_cust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PROSPECTID              0.0\n",
       "Total_TL                0.0\n",
       "Age_Oldest_TL           0.0\n",
       "Other_TL                0.0\n",
       "Unsecured_TL            0.0\n",
       "Secured_TL              0.0\n",
       "PL_TL                   0.0\n",
       "Home_TL                 0.0\n",
       "Gold_TL                 0.0\n",
       "Consumer_TL             0.0\n",
       "CC_TL                   0.0\n",
       "Auto_TL                 0.0\n",
       "Tot_Missed_Pmnt         0.0\n",
       "pct_tl_closed_L12M      0.0\n",
       "pct_tl_open_L12M        0.0\n",
       "Tot_TL_closed_L12M      0.0\n",
       "Total_TL_opened_L12M    0.0\n",
       "pct_closed_tl           0.0\n",
       "pct_active_tl           0.0\n",
       "pct_tl_closed_L6M       0.0\n",
       "pct_tl_open_L6M         0.0\n",
       "Tot_TL_closed_L6M       0.0\n",
       "Total_TL_opened_L6M     0.0\n",
       "Tot_Active_TL           0.0\n",
       "Tot_Closed_TL           0.0\n",
       "Age_Newest_TL           0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((df_internal_data_cust==-99999).sum()/len(df_internal_data_cust)*100).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PROSPECTID                    0.0\n",
       "pct_opened_TLs_L6m_of_L12m    0.0\n",
       "time_since_recent_enq         0.0\n",
       "enq_L12m                      0.0\n",
       "enq_L6m                       0.0\n",
       "enq_L3m                       0.0\n",
       "MARITALSTATUS                 0.0\n",
       "EDUCATION                     0.0\n",
       "AGE                           0.0\n",
       "GENDER                        0.0\n",
       "NETMONTHLYINCOME              0.0\n",
       "Time_With_Curr_Empr           0.0\n",
       "pct_of_active_TLs_ever        0.0\n",
       "pct_currentBal_all_TL         0.0\n",
       "time_since_recent_payment     0.0\n",
       "CC_Flag                       0.0\n",
       "PL_Flag                       0.0\n",
       "pct_PL_enq_L6m_of_L12m        0.0\n",
       "pct_CC_enq_L6m_of_L12m        0.0\n",
       "pct_PL_enq_L6m_of_ever        0.0\n",
       "pct_CC_enq_L6m_of_ever        0.0\n",
       "HL_Flag                       0.0\n",
       "GL_Flag                       0.0\n",
       "last_prod_enq2                0.0\n",
       "first_prod_enq2               0.0\n",
       "Credit_Score                  0.0\n",
       "PL_enq_L12m                   0.0\n",
       "PL_enq_L6m                    0.0\n",
       "PL_enq                        0.0\n",
       "CC_enq_L12m                   0.0\n",
       "num_times_delinquent          0.0\n",
       "max_recent_level_of_deliq     0.0\n",
       "num_deliq_6mts                0.0\n",
       "num_deliq_12mts               0.0\n",
       "num_deliq_6_12mts             0.0\n",
       "num_times_30p_dpd             0.0\n",
       "num_times_60p_dpd             0.0\n",
       "num_std                       0.0\n",
       "num_std_6mts                  0.0\n",
       "num_std_12mts                 0.0\n",
       "num_sub                       0.0\n",
       "num_sub_6mts                  0.0\n",
       "num_sub_12mts                 0.0\n",
       "num_dbt                       0.0\n",
       "num_dbt_6mts                  0.0\n",
       "num_dbt_12mts                 0.0\n",
       "num_lss                       0.0\n",
       "num_lss_6mts                  0.0\n",
       "num_lss_12mts                 0.0\n",
       "recent_level_of_deliq         0.0\n",
       "tot_enq                       0.0\n",
       "CC_enq                        0.0\n",
       "CC_enq_L6m                    0.0\n",
       "Approved_Flag                 0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((df_cibil_data_cust==-99999).sum()/len(df_cibil_data_cust)*100).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Merge the two dataframes, inner join so that no nulls are present\n",
    "def merge_df(df1,df2):\n",
    "     df = pd. merge ( df1, df2, how ='inner', left_on = ['PROSPECTID'], right_on = ['PROSPECTID'] )\n",
    "     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_data_cust=merge_df(df_internal_data_cust,df_cibil_data_cust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42064, 79)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_data_cust.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create train,validation and test datasets\n",
    "def split_data(df, train_size=0.7, val_size=0.15, test_size=0.15, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into train, validation, and test sets.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The full dataset.\n",
    "        target_column (str): The name of the target column.\n",
    "        train_size (float): Proportion of the dataset for training (default: 70%).\n",
    "        val_size (float): Proportion for validation (default: 15%).\n",
    "        test_size (float): Proportion for testing (default: 15%).\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        df_train (pd.DataFrame): Training dataset.\n",
    "        df_validation (pd.DataFrame): Validation dataset.\n",
    "        df_test (pd.DataFrame): Testing dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the split sizes sum to 1\n",
    "    assert train_size + val_size + test_size == 1, \"train, val, and test sizes must sum to 1\"\n",
    "    \n",
    "    # First, split into train and temp (validation + test)\n",
    "    df_train, df_temp = train_test_split(df, test_size=(val_size + test_size), random_state=random_state)\n",
    "\n",
    "    # Then, split temp into validation and test\n",
    "    df_validation, df_test = train_test_split(df_temp, test_size=(test_size / (val_size + test_size)), random_state=random_state)\n",
    "    \n",
    "    return df_train, df_validation, df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(df, train_size=0.8, val_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into training and validation sets.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The full dataset.\n",
    "        train_size (float): Proportion of the dataset for training (default: 80%).\n",
    "        val_size (float): Proportion for validation (default: 20%).\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        df_train (pd.DataFrame): Training dataset.\n",
    "        df_validation (pd.DataFrame): Validation dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the split sizes sum to 1\n",
    "    assert train_size + val_size == 1, \"train and validation sizes must sum to 1\"\n",
    "    \n",
    "    # Split into train and validation\n",
    "    df_train, df_validation = train_test_split(df, test_size=val_size, random_state=random_state)\n",
    "    \n",
    "    return df_train, df_validation\n",
    "\n",
    "# Example usage\n",
    "# df_train, df_validation = split_train_validation(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,df_validation=split_data(df_final_data_cust,train_size=0.8, val_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PROSPECTID</th>\n",
       "      <th>Total_TL</th>\n",
       "      <th>Tot_Closed_TL</th>\n",
       "      <th>Tot_Active_TL</th>\n",
       "      <th>Total_TL_opened_L6M</th>\n",
       "      <th>Tot_TL_closed_L6M</th>\n",
       "      <th>pct_tl_open_L6M</th>\n",
       "      <th>pct_tl_closed_L6M</th>\n",
       "      <th>pct_active_tl</th>\n",
       "      <th>pct_closed_tl</th>\n",
       "      <th>...</th>\n",
       "      <th>pct_PL_enq_L6m_of_L12m</th>\n",
       "      <th>pct_CC_enq_L6m_of_L12m</th>\n",
       "      <th>pct_PL_enq_L6m_of_ever</th>\n",
       "      <th>pct_CC_enq_L6m_of_ever</th>\n",
       "      <th>HL_Flag</th>\n",
       "      <th>GL_Flag</th>\n",
       "      <th>last_prod_enq2</th>\n",
       "      <th>first_prod_enq2</th>\n",
       "      <th>Credit_Score</th>\n",
       "      <th>Approved_Flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1357</th>\n",
       "      <td>1630</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>ConsumerLoan</td>\n",
       "      <td>ConsumerLoan</td>\n",
       "      <td>677</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37291</th>\n",
       "      <td>45501</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ConsumerLoan</td>\n",
       "      <td>others</td>\n",
       "      <td>682</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>842</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PL</td>\n",
       "      <td>others</td>\n",
       "      <td>684</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15696</th>\n",
       "      <td>19126</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ConsumerLoan</td>\n",
       "      <td>ConsumerLoan</td>\n",
       "      <td>673</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19503</th>\n",
       "      <td>23827</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>others</td>\n",
       "      <td>others</td>\n",
       "      <td>700</td>\n",
       "      <td>P2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PROSPECTID  Total_TL  Tot_Closed_TL  Tot_Active_TL  \\\n",
       "1357         1630         4              2              2   \n",
       "37291       45501         1              1              0   \n",
       "701           842         2              0              2   \n",
       "15696       19126         1              1              0   \n",
       "19503       23827         1              1              0   \n",
       "\n",
       "       Total_TL_opened_L6M  Tot_TL_closed_L6M  pct_tl_open_L6M  \\\n",
       "1357                     0                  1              0.0   \n",
       "37291                    0                  0              0.0   \n",
       "701                      0                  0              0.0   \n",
       "15696                    0                  1              0.0   \n",
       "19503                    0                  0              0.0   \n",
       "\n",
       "       pct_tl_closed_L6M  pct_active_tl  pct_closed_tl  ...  \\\n",
       "1357                0.25            0.5            0.5  ...   \n",
       "37291               0.00            0.0            1.0  ...   \n",
       "701                 0.00            1.0            0.0  ...   \n",
       "15696               1.00            0.0            1.0  ...   \n",
       "19503               0.00            0.0            1.0  ...   \n",
       "\n",
       "       pct_PL_enq_L6m_of_L12m  pct_CC_enq_L6m_of_L12m  pct_PL_enq_L6m_of_ever  \\\n",
       "1357                      0.0                     0.0                     0.0   \n",
       "37291                     0.0                     0.0                     0.0   \n",
       "701                       1.0                     0.0                     1.0   \n",
       "15696                     0.0                     0.0                     0.0   \n",
       "19503                     0.0                     0.0                     0.0   \n",
       "\n",
       "       pct_CC_enq_L6m_of_ever  HL_Flag  GL_Flag  last_prod_enq2  \\\n",
       "1357                      0.0        1        0    ConsumerLoan   \n",
       "37291                     0.0        0        0    ConsumerLoan   \n",
       "701                       0.0        0        0              PL   \n",
       "15696                     0.0        0        0    ConsumerLoan   \n",
       "19503                     0.0        0        0          others   \n",
       "\n",
       "       first_prod_enq2  Credit_Score  Approved_Flag  \n",
       "1357      ConsumerLoan           677             P2  \n",
       "37291           others           682             P2  \n",
       "701             others           684             P2  \n",
       "15696     ConsumerLoan           673             P2  \n",
       "19503           others           700             P2  \n",
       "\n",
       "[5 rows x 79 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((29444, 79), (6310, 79), (6310, 79))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape,df_validation.shape,df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### store the data to train_data,test_Data,validation data for later for our feature encoding and model_building\n",
    "def save_df_dir(df,dir):\n",
    "    #convert df to csv\n",
    "    df=df.reset_index(drop=True)\n",
    "    df.to_csv(dir, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file=root_folder_path/'data'/'train_data'/'cust_train_data.csv'\n",
    "validation_data_file =root_folder_path/'data'/'validation_data'/'cust_val_data.csv'\n",
    "test_data_file =root_folder_path/'data'/'test_data'/'cust_test_data.csv'\n",
    "save_df_dir(df_train,train_data_file)\n",
    "save_df_dir(df_validation,validation_data_file)\n",
    "save_df_dir(df_test,test_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### We willl create one function that will do all the feature selection in one go and return the selected features data freame\n",
    "\n",
    "def feature_selection_data(df_final_data_cust):\n",
    "    # Chi-square test\n",
    "    cat_columns_chi2 = []\n",
    "    for i in ['MARITALSTATUS', 'EDUCATION', 'GENDER', 'last_prod_enq2', 'first_prod_enq2']:\n",
    "        chi2, pval, _, _ = chi2_contingency(pd.crosstab(df_final_data_cust[i], df_final_data_cust['Approved_Flag']))\n",
    "        if pval <=0.05:\n",
    "            cat_columns_chi2.append(i)\n",
    "\n",
    "    # Apply sequential VIf for nyumerical columns\n",
    "    # numerical columns list\n",
    "    numeric_columns = []\n",
    "    columns_to_be_kept = []\n",
    "    for i in df_final_data_cust.columns:\n",
    "        if df_final_data_cust[i].dtype != 'object' and i not in ['PROSPECTID','Approved_Flag']:\n",
    "            numeric_columns.append(i)\n",
    "    vif_data = df_final_data_cust[numeric_columns]\n",
    "    total_columns = vif_data.shape[1]\n",
    "    column_index = 0\n",
    "    for i in range (0,total_columns):  \n",
    "        vif_value = variance_inflation_factor(vif_data, column_index)\n",
    "        # print (column_index,'---',vif_value)\n",
    "        if vif_value <= 6:\n",
    "            columns_to_be_kept.append( numeric_columns[i] )\n",
    "            column_index = column_index+1\n",
    "        else:\n",
    "            vif_data = vif_data.drop([ numeric_columns[i] ] , axis=1)\n",
    "    # check Anova for columns_to_be_kept \n",
    "    from scipy.stats import f_oneway\n",
    "    columns_to_be_kept_numerical = []\n",
    "    for i in columns_to_be_kept:\n",
    "        a = list(df_final_data_cust[i])  \n",
    "        b = list(df_final_data_cust['Approved_Flag'])     \n",
    "        group_P1 = [value for value, group in zip(a, b) if group == 'P1']\n",
    "        group_P2 = [value for value, group in zip(a, b) if group == 'P2']\n",
    "        group_P3 = [value for value, group in zip(a, b) if group == 'P3']\n",
    "        group_P4 = [value for value, group in zip(a, b) if group == 'P4']\n",
    "        f_statistic, p_value = f_oneway(group_P1, group_P2, group_P3, group_P4)\n",
    "        if p_value <= 0.05:\n",
    "            columns_to_be_kept_numerical.append(i)\n",
    "    #Combine all\n",
    "    selected_features_list_without_target=columns_to_be_kept_numerical + cat_columns_chi2\n",
    "    #final_features_df=df_final_data_cust[features_list_without_target + ['Approved_Flag']]\n",
    "    return selected_features_list_without_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features_to_json(features_list, dir_path, filename=\"features.json\"):\n",
    "    \"\"\"\n",
    "    Saves a list of features into a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "        features_list (list): List of feature names.\n",
    "        dir_path (str or Path): Directory where the JSON file should be saved.\n",
    "        filename (str): Name of the JSON file (default: 'features.json').\n",
    "\n",
    "    Returns:\n",
    "        None (Saves JSON file in the given directory)\n",
    "    \"\"\"\n",
    "    dir_path = Path(dir_path)\n",
    "    os.makedirs(dir_path, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "    file_path = dir_path / filename  # Full file path\n",
    "\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json.dump(features_list, json_file, indent=4)  # Save list as JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29444, 79)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training_cust.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MARITALSTATUS',\n",
       " 'EDUCATION',\n",
       " 'GENDER',\n",
       " 'last_prod_enq2',\n",
       " 'first_prod_enq2',\n",
       " 'Approved_Flag']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_cols = df_training_cust.select_dtypes(include=['object']).columns.tolist()\n",
    "categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create features after feature selection\n",
    "def create_save_features_from_training(train_data_file,dir_path, filename):\n",
    "    #Read training data\n",
    "    df_training_cust=pd.read_csv(train_data_file)\n",
    "    # do the feature selection\n",
    "    selected_features_list_without_target=feature_selection_data(df_training_cust)\n",
    "    save_features_to_json(selected_features_list_without_target, dir_path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhiraj_choudhary\\AppData\\Local\\miniconda3\\envs\\risk_modelling_3.8\\lib\\site-packages\\statsmodels\\stats\\outliers_influence.py:198: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n",
      "c:\\Users\\dhiraj_choudhary\\AppData\\Local\\miniconda3\\envs\\risk_modelling_3.8\\lib\\site-packages\\statsmodels\\stats\\outliers_influence.py:198: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n",
      "c:\\Users\\dhiraj_choudhary\\AppData\\Local\\miniconda3\\envs\\risk_modelling_3.8\\lib\\site-packages\\statsmodels\\stats\\outliers_influence.py:198: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n",
      "c:\\Users\\dhiraj_choudhary\\AppData\\Local\\miniconda3\\envs\\risk_modelling_3.8\\lib\\site-packages\\statsmodels\\stats\\outliers_influence.py:198: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n",
      "c:\\Users\\dhiraj_choudhary\\AppData\\Local\\miniconda3\\envs\\risk_modelling_3.8\\lib\\site-packages\\statsmodels\\stats\\outliers_influence.py:198: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n"
     ]
    }
   ],
   "source": [
    "train_data_file=train_data_file\n",
    "dir_path=root_folder_path/'src'/'models'\n",
    "file_name='features_risk_modeling.json'\n",
    "create_save_features_from_training(train_data_file,dir_path,file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the save features \n",
    "\n",
    "def load_features_from_json(file_path):\n",
    "    \"\"\"\n",
    "    Reads a list of features from a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str or Path): Path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        list: List of feature names.\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "\n",
    "    if not file_path.exists():\n",
    "        print(f\"Error: File {file_path} not found!\")\n",
    "        return []\n",
    "\n",
    "    with open(file_path, \"r\") as json_file:\n",
    "        features_list = json.load(json_file)  # Load JSON file\n",
    "\n",
    "    print(f\"Features loaded successfully from: {file_path}\")\n",
    "    return features_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features loaded successfully from: c:\\Users\\dhiraj_choudhary\\Music\\Credit-Risk-Modelling\\src\\models\\features_risk_modeling.json\n"
     ]
    }
   ],
   "source": [
    "file_path=root_folder_path/'src'/'models'/'features_risk_modeling.json'\n",
    "features_list=load_features_from_json(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pct_tl_open_L6M',\n",
       " 'pct_tl_closed_L6M',\n",
       " 'Tot_TL_closed_L12M',\n",
       " 'pct_tl_closed_L12M',\n",
       " 'Tot_Missed_Pmnt',\n",
       " 'CC_TL',\n",
       " 'Home_TL',\n",
       " 'PL_TL',\n",
       " 'Secured_TL',\n",
       " 'Unsecured_TL',\n",
       " 'Other_TL',\n",
       " 'Age_Oldest_TL',\n",
       " 'Age_Newest_TL',\n",
       " 'time_since_recent_payment',\n",
       " 'max_recent_level_of_deliq',\n",
       " 'num_deliq_6_12mts',\n",
       " 'num_times_60p_dpd',\n",
       " 'num_std_12mts',\n",
       " 'num_sub',\n",
       " 'num_dbt',\n",
       " 'num_dbt_12mts',\n",
       " 'recent_level_of_deliq',\n",
       " 'CC_enq_L12m',\n",
       " 'PL_enq_L12m',\n",
       " 'time_since_recent_enq',\n",
       " 'enq_L3m',\n",
       " 'NETMONTHLYINCOME',\n",
       " 'Time_With_Curr_Empr',\n",
       " 'CC_Flag',\n",
       " 'PL_Flag',\n",
       " 'pct_PL_enq_L6m_of_ever',\n",
       " 'pct_CC_enq_L6m_of_ever',\n",
       " 'HL_Flag',\n",
       " 'GL_Flag',\n",
       " 'MARITALSTATUS',\n",
       " 'EDUCATION',\n",
       " 'GENDER',\n",
       " 'last_prod_enq2',\n",
       " 'first_prod_enq2']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######self modelarize block\n",
    "df_training_cust=pd.read_csv(train_data_file)\n",
    "df_training_cust_work_upon=df_training_cust[features_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_colwidth', None)  # Prevent column truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.curdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=df_training_cust_work_upon.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "with open('num_features.json', \"w\") as json_file:\n",
    "    json.dump(list1, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "list2=df_training_cust_work_upon.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "with open('cat_features.json', \"w\") as json_file:\n",
    "    json.dump(list2, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Got the feature list and we need to encode the features now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In training we will use feature list and will cretae the column transformer - and will encode the features and save the classifer\n",
    "later in training we will us the same classifier and will train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features loaded successfully from: c:\\Users\\dhiraj_choudhary\\Music\\Credit-Risk-Modelling\\src\\models\\features_risk_modeling.json\n",
      "Preprocessing pipeline for riskreporting saved succesfully\n"
     ]
    }
   ],
   "source": [
    "def create_save_feature_encoder_classifier(features_list,train_data_file,dir_path,file_name):\n",
    "    #train data frame\n",
    "    df_training_cust=pd.read_csv(train_data_file)\n",
    "    # select onnly features that we willl work on\n",
    "    df_training_cust_work_upon=df_training_cust[features_list]\n",
    "    # Identify numerical columns\n",
    "    numerical_cols_training_cust = df_training_cust_work_upon.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    # Identify categorical columns\n",
    "    categorical_cols_training_cust = df_training_cust_work_upon.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    # print(categorical_cols_training_cust)\n",
    "    #ordinal columns\n",
    "    categorical_cols_training_ordinal = ['EDUCATION']\n",
    "    categorical_cols_training_cust_non_ordinal = [col for col in categorical_cols_training_cust if col not in categorical_cols_training_ordinal]\n",
    "    # Define ordinal encoding categories for EDUCATION\n",
    "    education_categories = [['SSC', '12TH', 'UNDER GRADUATE', 'GRADUATE', 'OTHERS', 'POST-GRADUATE', 'PROFESSIONAL']]\n",
    "\n",
    "    # Create preprocessing pipeline\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('ordinal', OrdinalEncoder(categories=education_categories), ['EDUCATION']),  # Ordinal encoding for EDUCATION\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False),categorical_cols_training_cust_non_ordinal),  # One-hot encoding\n",
    "            ('scaler', StandardScaler(), numerical_cols_training_cust)  # Standardize numerical columns\n",
    "        ]\n",
    "    )\n",
    "    preprocessor.fit(df_training_cust_work_upon)\n",
    "    \n",
    "    # Save the pipeline\n",
    "    dir_path = Path(dir_path)\n",
    "    os.makedirs(dir_path, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "    file_path = dir_path / file_name  # Full file path\n",
    "    joblib.dump(preprocessor,file_path)\n",
    "\n",
    "    print(f\"Preprocessing pipeline for riskreporting saved succesfully\")\n",
    "\n",
    "\n",
    "#trigger the function\n",
    "features_list=load_features_from_json(file_path)\n",
    "train_data_file=train_data_file\n",
    "dir_path=root_folder_path/'src'/'models'\n",
    "file_name_preprocessor='preprocessing_pipeline_risk_modelling.pkl'\n",
    "\n",
    "\n",
    "create_save_feature_encoder_classifier(features_list,train_data_file,dir_path,file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39,\n",
       " WindowsPath('c:/Users/dhiraj_choudhary/Music/Credit-Risk-Modelling/data/train_data/cust_train_data.csv'),\n",
       " WindowsPath('c:/Users/dhiraj_choudhary/Music/Credit-Risk-Modelling/src/models'),\n",
       " 'preprocessing_pipeline_risk_modelling.pkl')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_list),train_data_file,dir_path,file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training pipeline and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Approved_Flag\n",
       "P2    17895\n",
       "P3     4465\n",
       "P4     3713\n",
       "P1     3371\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the input features and target variable\n",
    "train_data_file=root_folder_path/'data'/'train_data'/'cust_train_data.csv'\n",
    "df_training=pd.read_csv(train_data_file)\n",
    "X_train=df_training[features_list]\n",
    "y_train=df_training['Approved_Flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29444, 79)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6310, 79)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6310, 79)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Target label and \n",
    "\n",
    "def save_lable_encode(dir_path,file_name,y_train):\n",
    "    label_encoder_target=LabelEncoder()\n",
    "    label_encoder_target.fit(y_train)\n",
    "    file_path=dir_path/file_name\n",
    "    joblib.dump(label_encoder_target,file_path)\n",
    "\n",
    "dir_path=root_folder_path/'src'/'models'\n",
    "file_name_label_encoder='Label_Encode_Risk_modelling.pkl'\n",
    "\n",
    "# triiger the function\n",
    "save_lable_encode(dir_path,file_name_label_encoder,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We will use same label encoder and preprocessed pipe line for features to tarining the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exp-1 -Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(model,param_distributions,preprocessing_pipeline,label_encoder,X_train,y_train,save_dir_path,model_name):\n",
    "\n",
    "    #Encode the features/input\n",
    "    X_train_encoded=preprocessing_pipeline.transform(X_train)\n",
    "    #Encode the label/taregt\n",
    "    y_train_encoded=label_encoder.transform(y_train)\n",
    "    #fit the model\n",
    "    # Hyperparameter tuning using RandomizedSearchCV\n",
    "    search = RandomizedSearchCV(model, param_distributions=param_distributions, n_iter=4, cv=3, n_jobs=-1, random_state=42)\n",
    "    search.fit(X_train_encoded, y_train_encoded)\n",
    "\n",
    "    # Best model & hyperparameters\n",
    "    best_model = search.best_estimator_\n",
    "    best_params = search.best_params_\n",
    "\n",
    "    # Save model\n",
    "    model_path = os.path.join(save_dir_path, f\"{model_name}.pkl\")\n",
    "    joblib.dump(best_model, model_path)\n",
    "\n",
    "    # Training metric (accuracy)\n",
    "    y_train_pred = best_model.predict(X_train_encoded)\n",
    "    train_acc = accuracy_score(y_train_encoded, y_train_pred)\n",
    "\n",
    "    #log the experiment\n",
    "\n",
    "     # Experiment log\n",
    "    experiment_data = {\n",
    "        \"model_name\": model_name,\n",
    "        \"best_params\": best_params,\n",
    "        \"training_metric\": train_acc,\n",
    "        \"model_path\": model_path\n",
    "    }\n",
    "\n",
    "     # Save experiment log\n",
    "    exp_log_path = os.path.join(save_dir_path, f\"{model_name}_experiment.json\")\n",
    "    with open(exp_log_path, \"w\") as f:\n",
    "        json.dump(experiment_data, f, indent=4)\n",
    "\n",
    "    #log the few result\n",
    "    print(f\"Training complete! Best model saved: {model_path}\")\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "    return experiment_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'src/models/model_RandomForestClassifierpkl'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir_path=\"src/models/model_\"\n",
    "\n",
    "save_dir_path+'RandomForestClassifier'+'pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete! Best model saved: c:\\Users\\dhiraj_choudhary\\Music\\Credit-Risk-Modelling\\src\\experiments\\RandomForest.pkl\n",
      "Best Parameters: {'n_estimators': 200, 'min_samples_split': 10, 'max_depth': None}\n",
      "Training Accuracy: 0.9470\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_name': 'RandomForest',\n",
       " 'best_params': {'n_estimators': 200,\n",
       "  'min_samples_split': 10,\n",
       "  'max_depth': None},\n",
       " 'training_metric': 0.9469841054204592,\n",
       " 'model_path': 'c:\\\\Users\\\\dhiraj_choudhary\\\\Music\\\\Credit-Risk-Modelling\\\\src\\\\experiments\\\\RandomForest.pkl'}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#call the above function\n",
    "dir_path=root_folder_path/'src'/'models'\n",
    "file_name_label_encoder='Label_Encode_Risk_modelling.pkl'\n",
    "file_name_preprocessor='preprocessing_pipeline_risk_modelling.pkl'\n",
    "file_path_label_encoder=dir_path/file_name_label_encoder\n",
    "file_path_preprocessor=dir_path/file_name_preprocessor\n",
    "#parameters value of function\n",
    "label_encoder_risk_modelling=joblib.load(file_path_label_encoder)\n",
    "preprocessor_risk_model=joblib.load(file_path_preprocessor)\n",
    "dir_path_expr=root_folder_path/'src'/'experiments'\n",
    "# Define model & hyperparameter grid\n",
    "model_name='RandomForest'\n",
    "model = RandomForestClassifier()\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"max_depth\": [None, 10, 20],\n",
    "    \"min_samples_split\": [2, 5, 10]\n",
    "}\n",
    "\n",
    "training_model(model,param_grid,preprocessor_risk_model,label_encoder_risk_modelling,X_train,y_train,dir_path_expr,model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vlidation of models - we will use same label encoder and preprocessed pipeline of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the input features and target variable\n",
    "val_data_file=root_folder_path/'data'/'validation_data'/'cust_val_data.csv'\n",
    "df_validation=pd.read_csv(val_data_file)\n",
    "X_val=df_validation[features_list]\n",
    "y_val=df_validation['Approved_Flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6310, 79)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(preprocessing_pipeline,label_encoder,experiment_log, X_val, y_val,dir_path):\n",
    "    \"\"\"\n",
    "    Loads the best saved model and evaluates it on the validation dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - experiment_log: Path to the experiment JSON log.\n",
    "    - X_val: Validation features.\n",
    "    - y_val: Validation labels.\n",
    "\n",
    "    Returns:\n",
    "    - Validation accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    #Encode the features/input\n",
    "    X_val_encoded=preprocessing_pipeline.transform(X_val)\n",
    "    #Encode the label/taregt\n",
    "    y_val_encoded=label_encoder.transform(y_val)\n",
    "    # dir path\n",
    "    exp_log_path = os.path.join(dir_path, f\"{experiment_log}\")\n",
    "    # Load experiment details\n",
    "    with open(exp_log_path, \"r\") as f:\n",
    "        experiment_data = json.load(f)\n",
    "    \n",
    "    # Load saved best model\n",
    "    model_path = experiment_data[\"model_path\"]\n",
    "    best_model = joblib.load(model_path)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    y_val_pred = best_model.predict(X_val_encoded)\n",
    "    val_acc = accuracy_score(y_val_encoded, y_val_pred)\n",
    "\n",
    "    print(f\"Validation Accuracy: {val_acc:.4f} (Using best model: {experiment_data['model_name']})\")\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7623 (Using best model: RandomForest)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7622820919175911"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call the function\n",
    "dir_path=root_folder_path/'src'/'models'\n",
    "file_name_label_encoder='Label_Encode_Risk_modelling.pkl'\n",
    "file_name_preprocessor='preprocessing_pipeline_risk_modelling.pkl'\n",
    "file_path_label_encoder=dir_path/file_name_label_encoder\n",
    "file_path_preprocessor=dir_path/file_name_preprocessor\n",
    "#parameters value of function\n",
    "label_encoder_risk_modelling=joblib.load(file_path_label_encoder)\n",
    "preprocessor_risk_model=joblib.load(file_path_preprocessor)\n",
    "experiment_log='RandomForest_experiment.json'\n",
    "dir_path_expr=root_folder_path/'src'/'experiments'\n",
    "\n",
    "validate_model(preprocessor_risk_model,label_encoder_risk_modelling,experiment_log,X_val,y_val,dir_path_expr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exp-2 : Decission tree Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete! Best model saved: c:\\Users\\dhiraj_choudhary\\Music\\Credit-Risk-Modelling\\src\\experiments\\DecisionTree.pkl\n",
      "Best Parameters: {'min_samples_split': 10, 'max_depth': 10}\n",
      "Training Accuracy: 0.8065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_name': 'DecisionTree',\n",
       " 'best_params': {'min_samples_split': 10, 'max_depth': 10},\n",
       " 'training_metric': 0.8065140605895939,\n",
       " 'model_path': 'c:\\\\Users\\\\dhiraj_choudhary\\\\Music\\\\Credit-Risk-Modelling\\\\src\\\\experiments\\\\DecisionTree.pkl'}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#call the above function\n",
    "dir_path=root_folder_path/'src'/'models'\n",
    "file_name_label_encoder='Label_Encode_Risk_modelling.pkl'\n",
    "file_name_preprocessor='preprocessing_pipeline_risk_modelling.pkl'\n",
    "file_path_label_encoder=dir_path/file_name_label_encoder\n",
    "file_path_preprocessor=dir_path/file_name_preprocessor\n",
    "#parameters value of function\n",
    "label_encoder_risk_modelling=joblib.load(file_path_label_encoder)\n",
    "preprocessor_risk_model=joblib.load(file_path_preprocessor)\n",
    "dir_path_expr=root_folder_path/'src'/'experiments'\n",
    "# Define model & hyperparameter grid\n",
    "model_name='DecisionTree'\n",
    "model = DecisionTreeClassifier()\n",
    "param_grid = {\n",
    "    \"max_depth\": [None, 10, 20],\n",
    "    \"min_samples_split\": [2, 5, 10]\n",
    "}\n",
    "\n",
    "training_model(model,param_grid,preprocessor_risk_model,label_encoder_risk_modelling,X_train,y_train,dir_path_expr,model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation/Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7471 (Using best model: DecisionTree)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.747068145800317"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call the function\n",
    "dir_path=root_folder_path/'src'/'models'\n",
    "file_name_label_encoder='Label_Encode_Risk_modelling.pkl'\n",
    "file_name_preprocessor='preprocessing_pipeline_risk_modelling.pkl'\n",
    "file_path_label_encoder=dir_path/file_name_label_encoder\n",
    "file_path_preprocessor=dir_path/file_name_preprocessor\n",
    "#parameters value of function\n",
    "label_encoder_risk_modelling=joblib.load(file_path_label_encoder)\n",
    "preprocessor_risk_model=joblib.load(file_path_preprocessor)\n",
    "experiment_log='DecisionTree_experiment.json'\n",
    "dir_path_expr=root_folder_path/'src'/'experiments'\n",
    "\n",
    "validate_model(preprocessor_risk_model,label_encoder_risk_modelling,experiment_log,X_val,y_val,dir_path_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "risk_modelling_3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
